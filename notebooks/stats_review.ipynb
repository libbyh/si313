{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:33:57.768988Z",
     "start_time": "2025-09-01T17:33:57.523118Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e382acab7c94db",
   "metadata": {},
   "source": [
    "# Get Data\n",
    "We're using data from the [Stress in America survey](https://www.icpsr.umich.edu/web/ICPSR/studies/37288), specifically the 2016 Coping with Change, Part 2: Technology and Social Media wave. You will need an account at ICPSR to download the data to follow along. _You will not be able to run this notebook without downloading the data from ICPSR._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69306d2db40cc54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:33:57.949658Z",
     "start_time": "2025-09-01T17:33:57.771301Z"
    }
   },
   "outputs": [],
   "source": [
    "# read in the delimited file\n",
    "df = pd.read_csv('~/Downloads/ICPSR_37288/DS0012/37288-0012-Data.tsv', sep='\\t')\n",
    "print(len(df.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f806ecc7510fa5e",
   "metadata": {},
   "source": [
    "The error tells us something is up with column 163. Let's investigate and see if we can fix the problem. Or whether it's even a problem we need to fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610abfa570eb142b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:33:58.018047Z",
     "start_time": "2025-09-01T17:33:58.015924Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the name of the column at index 163\n",
    "column_name = df.columns[163]\n",
    "\n",
    "# Apply value_counts() to the column\n",
    "frequencies = df[column_name].value_counts(dropna=False)\n",
    "\n",
    "# Print the result\n",
    "print(frequencies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d599a7b3de3a1d",
   "metadata": {},
   "source": [
    "We can go to the codebook to figure out what this question is about and what those answers mean. Then we'll figure out why we see values that look the same but that `pandas` is reading as different (e.g., 4 and 4).\n",
    "\n",
    "The NETRACEUS variable looks like it collapses the many race categories offered earlier in the survey into just 4 groups. This is a categorical variable, so we can use the categories themselves as values instead fo the number to make it more readable.\n",
    "\n",
    "First, let's make all the values strings since it looks like there's inconsistency in the data type for the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915f23cfceec3cd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:33:58.025821Z",
     "start_time": "2025-09-01T17:33:58.023382Z"
    }
   },
   "outputs": [],
   "source": [
    "df[column_name] = df[column_name].astype(str)\n",
    "\n",
    "# Now, apply value_counts()\n",
    "frequencies = df[column_name].value_counts(dropna=False)\n",
    "print(frequencies)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38655ac6a9571fa9",
   "metadata": {},
   "source": [
    "That gets us the 4 categories we were expecting (plus some missing data). Let's recode those to match the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e56c7cb64c658b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:33:58.035026Z",
     "start_time": "2025-09-01T17:33:58.031754Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to map the old values to the new values\n",
    "recoding_dict = {\n",
    "    '1': 'Hispanic',\n",
    "    '2': 'Black (not Hispanic)',\n",
    "    '3': 'Asian (not Hispanic)',\n",
    "    '4': 'All other (not Hispanic)'\n",
    "}\n",
    "\n",
    "# Apply the recoding\n",
    "df[column_name] = df[column_name].replace(recoding_dict)\n",
    "\n",
    "# Print the value counts again to verify the changes\n",
    "print(df[column_name].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c1f3bdd485d6cf",
   "metadata": {},
   "source": [
    "There may be other issues with the data that we discover as we go, but at least we have the read error fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4defaa92eeaab66",
   "metadata": {},
   "source": [
    "# Explore Data\n",
    "\n",
    "Before we get much further, let's take a look at some of the variables we might use in this data. The textbook has questions on p. 74 that are a good starting point. We're not going to use all 4051 variables. Let's first choose some to keep.\n",
    "\n",
    "I noticed while prepping this demo that many of the variables in the dataset have *only* missing values. Let's start by getting rid of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525e0e3d68dac806",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:33:58.815700Z",
     "start_time": "2025-09-01T17:33:58.041707Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a boolean series where True means the column is entirely empty\n",
    "is_empty = ((df.replace(r'^\\s*$', np.nan, regex=True)\n",
    "             .isnull()\n",
    "             .all()\n",
    "             ))\n",
    "\n",
    "# Get the names of the columns that are all empty\n",
    "empty_columns = list(is_empty[is_empty].index)\n",
    "\n",
    "print(\"N of columns that are all empty or full of whitespace:\")\n",
    "print(len(empty_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57c068c0b1204a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:33:58.825688Z",
     "start_time": "2025-09-01T17:33:58.823187Z"
    }
   },
   "outputs": [],
   "source": [
    "df_usable = df.drop(columns=empty_columns)\n",
    "print(len(df_usable))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaffc2ed4770b6d",
   "metadata": {},
   "source": [
    "We're left with only 1019 columns. That seems odd, so I messaged the ICPSR team to figure out what's up with this dataset. Luckily we're just reviewing stats and not actually using the data to make decisions, so we'll press on.\n",
    "\n",
    "Let's see which of those remaining variables might be interesting to work with. We have too many variables for Jupyter to display, so we have to use a loop to print them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67877c3c3314bcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:33:58.835964Z",
     "start_time": "2025-09-01T17:33:58.833554Z"
    }
   },
   "outputs": [],
   "source": [
    "for column in df_usable.columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a3d663b16d00f",
   "metadata": {},
   "source": [
    "Some interesting variables:\n",
    "- Q625: For each one, please indicate how significant a source of stress it is in your life (1-4, Not at all significant - Very significant)\n",
    "- Q810: Which of the following, if any, have you experienced in the last month as a result of\n",
    "stress? (List of things, e.g., headache)\n",
    "- Q3051: How would you rate your neighborhood on the following? (1-4, A big problem - Not a problem)\n",
    "- Q350:\n",
    "\n",
    "Many of the other variables are demographic (e.g., Male: how do you describe yourself?). For purposes of class, I'm going to exclude those. One really common step in data analysis is to segment data according to demographic variables. Doing so helps us identify disparities, for instance. But today we're focusing on reviewing steps to analyzing data, so we're going to avoid the pitfalls of reductionist views on demographics. We're going to keep just one so that I can demo looking for covariates though: NETAGE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d787e97c41db0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:33:58.848278Z",
     "start_time": "2025-09-01T17:33:58.843020Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a list of the column names you want to keep\n",
    "demog_columns_to_keep = ['NETAGE']\n",
    "\n",
    "# Add columns from Q9015:\n",
    "outcome_columns_to_keep = [col for col in df.columns if\n",
    "                           col.startswith('Q625') or col.startswith('Q810') or col.startswith('Q3051')]\n",
    "\n",
    "# Combine the lists of columns\n",
    "columns_to_keep = demog_columns_to_keep + outcome_columns_to_keep\n",
    "\n",
    "# Create a new DataFrame with just those columns\n",
    "df_subset = df[columns_to_keep]\n",
    "\n",
    "# To modify the original DataFrame in place, you can re-assign it\n",
    "df = df[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859196825a341848",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:33:58.861217Z",
     "start_time": "2025-09-01T17:33:58.855536Z"
    }
   },
   "outputs": [],
   "source": [
    "# show the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c33a40a2abe6d35",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:33:58.886060Z",
     "start_time": "2025-09-01T17:33:58.884513Z"
    }
   },
   "outputs": [],
   "source": [
    "for column in df.columns:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6a25c7b94d394",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:33:58.910763Z",
     "start_time": "2025-09-01T17:33:58.908861Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to map old names to new names\n",
    "rename_dict = {\n",
    "    'q3015_a': 'brand_loyalty',\n",
    "    'q3015_b': 'customer_satisfaction'\n",
    "}\n",
    "\n",
    "# Use the .rename() method to apply the changes\n",
    "df.rename(columns=rename_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e3c9775584c64c",
   "metadata": {},
   "source": [
    "I might recode a lot of variables because I like to work with labels. So, here's a function to do that repeatedly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb432caa7da07d42",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:33:58.949930Z",
     "start_time": "2025-09-01T17:33:58.947597Z"
    }
   },
   "outputs": [],
   "source": [
    "def recode_columns(df, columns_list, recoding_dict):\n",
    "    \"\"\"\n",
    "    Recodes values in a list of specified DataFrame columns based on a dictionary,\n",
    "    then prints the value counts of each modified column.\n",
    "\n",
    "    This function is optimized to apply the recoding to all columns at once.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to be modified.\n",
    "        columns_list (list): A list of column names to recode.\n",
    "        recoding_dict (dict): A dictionary where keys are original values\n",
    "                              and values are the new, recoded values.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with the recoded columns.\n",
    "    \"\"\"\n",
    "    print(\"Recoding these columns: \")\n",
    "    print(columns_list)\n",
    "    # Explicitly convert the columns to a string type to avoid a FutureWarning\n",
    "    # This is an important step to ensure consistency when mixing types like int and str.\n",
    "    df[columns_list] = df[columns_list].astype(str)\n",
    "\n",
    "    # Apply the recoding to all specified columns at once\n",
    "    df.loc[:, columns_list] = df[columns_list].replace(recoding_dict)\n",
    "\n",
    "    # Print the value counts for each recoded column to verify the changes\n",
    "    for column_name in columns_list:\n",
    "        print(f\"\\nValue counts for '{column_name}' after recoding:\")\n",
    "        print(df[column_name].value_counts(dropna=False))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944afb3b7db2829b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:33:58.984172Z",
     "start_time": "2025-09-01T17:33:58.980519Z"
    }
   },
   "outputs": [],
   "source": [
    "# recode to age variable\n",
    "recoding_dict = {\n",
    "    '1': '18-24',\n",
    "    '2': '25-34',\n",
    "    '3': '35-44',\n",
    "    '4': '45-54',\n",
    "    '5': '55-64',\n",
    "    '6': '65+',\n",
    "}\n",
    "\n",
    "df = recode_columns(df, ['NETAGE'], recoding_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e160f283f109c0d4",
   "metadata": {},
   "source": [
    "I notice that the scales for Q610 and Q3051 are reversed (higher isn't always more), and I'm going to change that too.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3778e39bc9a7cc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:33:58.988281Z",
     "start_time": "2025-09-01T17:33:58.986190Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_histograms(df, columns_list):\n",
    "    \"\"\"\n",
    "    Generates and displays a histogram for each specified column in the DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        columns_list (list): A list of column names to plot.\n",
    "    \"\"\"\n",
    "    # print(df.columns) # was for debugging\n",
    "    print(\"Generating histograms...\")\n",
    "    for column in columns_list:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sorted_values = pd.Categorical(df[column], ordered=True, categories=sorted(df[column].unique()))\n",
    "        sns.histplot(sorted_values, discrete=True, shrink=0.8)\n",
    "        plt.title(f\"Distribution of '{column}'\")\n",
    "        plt.xlabel(\"Value\")\n",
    "        plt.ylabel(\"Frequency\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0927e59507df6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:33:59.109077Z",
     "start_time": "2025-09-01T17:33:59.003325Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_histograms(df, ['NETAGE', 'Q3051_1'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d7eb381c294787",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:33:59.125632Z",
     "start_time": "2025-09-01T17:33:59.116218Z"
    }
   },
   "outputs": [],
   "source": [
    "recoding_dict = {\n",
    "    '1': '4',\n",
    "    '2': '3',\n",
    "    '3': '2',\n",
    "    '4': '1'\n",
    "}\n",
    "\n",
    "columns_to_recode = [col for col in df.columns if col.startswith('Q3051')]\n",
    "\n",
    "df = recode_columns(df, columns_to_recode, recoding_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec30a9ae23a18146",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:33:59.171674Z",
     "start_time": "2025-09-01T17:33:59.133876Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the histogram of the recoded variable to see if recoding worked\n",
    "plot_histograms(df, ['Q3051_1'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1ee9f15659b476",
   "metadata": {},
   "source": [
    "Now we've seen that the recode was successful. Let's see how a couple of different variables relate to each other.\n",
    "\n",
    "We'll show some two-way plots that show the combined frequency of two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f712e60145da3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:33:59.183019Z",
     "start_time": "2025-09-01T17:33:59.180887Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_two_way_distribution(df, col1, col2):\n",
    "    \"\"\"\n",
    "    Generates and displays a two-way frequency plot for two specified columns.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        col1 (str): The name of the first column for the plot.\n",
    "        col2 (str): The name of the second column for the plot.\n",
    "    \"\"\"\n",
    "    print(f\"\\nGenerating two-way distribution for '{col1}' and '{col2}'...\")\n",
    "\n",
    "    # Generate the cross-tabulation table to see the raw counts\n",
    "    cross_tab = pd.crosstab(df[col1], df[col2])\n",
    "    print(\"\\nTwo-Way Frequency Table:\")\n",
    "    print(cross_tab)\n",
    "\n",
    "    # Plot the data using a clustered bar chart\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.countplot(x=col1, hue=col2, data=df, palette=\"viridis\")\n",
    "    plt.title(f\"Two-Way Distribution of '{col1}' and '{col2}'\")\n",
    "    plt.xlabel(col1)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend(title=col2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991a6d862cc6c2fa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:33:59.316306Z",
     "start_time": "2025-09-01T17:33:59.197388Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_two_way_distribution(df, 'NETAGE', 'Q625R1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3f1b25b89e928e",
   "metadata": {},
   "source": [
    "Another approach the textbook recommends is to draw a scatterplot matrix. Our data won't be that interesting in scatterplots because we have categorical rather than continuous variables. So we use a bubble chart instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac612f0414c9b92",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:33:59.331528Z",
     "start_time": "2025-09-01T17:33:59.329135Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_bubble_chart_by_frequency(df, x_col, y_col):\n",
    "    \"\"\"\n",
    "    Generates a bubble chart where the size of the dots represents the\n",
    "    frequency of each unique combination of (x, y) values.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        x_col (str): The name of the column for the x-axis.\n",
    "        y_col (str): The name of the column for the y-axis.\n",
    "    \"\"\"\n",
    "    # 1. Count the occurrences of each unique combination\n",
    "    frequency_counts = df.groupby([x_col, y_col]).size().reset_index(name='frequency')\n",
    "\n",
    "    # 2. Plot the data using the 'size' aesthetic for frequency\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Use the seaborn scatterplot to easily map frequency to dot size\n",
    "    sns.scatterplot(\n",
    "        data=frequency_counts,\n",
    "        x=x_col,\n",
    "        y=y_col,\n",
    "        size='frequency',  # This is the key part that sizes the dots by frequency\n",
    "        sizes=(20, 400),  # Adjusts the range of dot sizes\n",
    "        alpha=0.8,\n",
    "        legend='brief'\n",
    "    )\n",
    "\n",
    "    plt.title(f\"Bubble Chart of {x_col} vs. {y_col} (Sized by Frequency)\")\n",
    "    plt.xlabel(x_col)\n",
    "    plt.ylabel(y_col)\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b180d0cbafd8c52",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:33:59.336580Z",
     "start_time": "2025-09-01T17:33:59.333694Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_bubble_grid(df, variables):\n",
    "    \"\"\"\n",
    "    Generates a grid of bubble charts for all unique pairs of variables.\n",
    "    The size of each dot represents the frequency of that (x, y) combination.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        variables (list): A list of column names to include in the grid.\n",
    "    \"\"\"\n",
    "    # Create a new DataFrame with only the selected variables\n",
    "    df_subset = df[variables]\n",
    "\n",
    "    # Generate all unique pairs of variables\n",
    "    import itertools\n",
    "    pairs = list(itertools.combinations(variables, 2))\n",
    "\n",
    "    num_plots = len(pairs)\n",
    "    num_cols = min(3, num_plots)\n",
    "    num_rows = (num_plots + num_cols - 1) // num_cols\n",
    "\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(5 * num_cols, 5 * num_rows))\n",
    "    axes = axes.flatten() if num_plots > 1 else [axes]\n",
    "\n",
    "    for i, (x_col, y_col) in enumerate(pairs):\n",
    "        # Count the occurrences of each unique combination\n",
    "        frequency_counts = df_subset.groupby([x_col, y_col]).size().reset_index(name='frequency')\n",
    "\n",
    "        # Use a single scatterplot for each subplot\n",
    "        sns.scatterplot(\n",
    "            data=frequency_counts,\n",
    "            x=x_col,\n",
    "            y=y_col,\n",
    "            size='frequency',\n",
    "            sizes=(20, 400),\n",
    "            alpha=0.8,\n",
    "            legend=False,\n",
    "            ax=axes[i]\n",
    "        )\n",
    "        axes[i].set_title(f\"{x_col} vs. {y_col}\")\n",
    "        axes[i].set_xlabel(x_col)\n",
    "        axes[i].set_ylabel(y_col)\n",
    "        axes[i].grid(True)\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for j in range(num_plots, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    plt.suptitle(\"Grid of Bubble Charts (Sized by Frequency)\", fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665ad529c5dda152",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:34:05.508723Z",
     "start_time": "2025-09-01T17:33:59.344837Z"
    }
   },
   "outputs": [],
   "source": [
    "# Select the columns you want to include in the scatterplot matrix\n",
    "stressors = [col for col in df.columns if col.startswith('Q625AR')]\n",
    "neighborhood = [col for col in df.columns if col.startswith('Q3051')]\n",
    "columns_to_plot = stressors + neighborhood\n",
    "\n",
    "# Plot a bubble chart where dot size represents frequency\n",
    "plot_bubble_chart_by_frequency(df, 'Q625AR1', 'Q3051_1')\n",
    "\n",
    "# Plot a grid of bubble charts for multiple variables\n",
    "plot_bubble_grid(df, columns_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aee21dd6168da89",
   "metadata": {},
   "source": [
    "# Inferential Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59cbc54c44b859b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:34:05.532498Z",
     "start_time": "2025-09-01T17:34:05.529885Z"
    }
   },
   "outputs": [],
   "source": [
    "def independent_t_test(df, numerical_var, grouping_var):\n",
    "    \"\"\"\n",
    "    Performs an independent samples t-test to compare the means of a\n",
    "    numerical variable between two groups.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        numerical_var (str): The name of the numerical variable to test.\n",
    "        grouping_var (str): The name of the grouping variable with two categories.\n",
    "    \"\"\"\n",
    "    print(f\"--- Independent T-Test: {numerical_var} by {grouping_var} ---\")\n",
    "\n",
    "    # Filter data for the two groups\n",
    "    group_1 = df[df[grouping_var] == df[grouping_var].unique()[0]][numerical_var].dropna()\n",
    "    group_2 = df[df[grouping_var] == df[grouping_var].unique()[1]][numerical_var].dropna()\n",
    "\n",
    "    # Calculate and print group means\n",
    "    print(f\"Mean for {df[grouping_var].unique()[0]}: {group_1.mean():.4f}\")\n",
    "    print(f\"Mean for {df[grouping_var].unique()[1]}: {group_2.mean():.4f}\")\n",
    "\n",
    "    # Perform the t-test\n",
    "    t_stat, p_value = stats.ttest_ind(group_1, group_2, equal_var=False)  # Assumes unequal variances\n",
    "\n",
    "    # Print results and interpretation\n",
    "    print(f\"T-Statistic: {t_stat:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "    alpha = 0.05\n",
    "    if p_value < alpha:\n",
    "        print(\"\\nResult: The difference in means is statistically significant.\")\n",
    "        print(\"This suggests that the two groups are different on this variable.\")\n",
    "    else:\n",
    "        print(\"\\nResult: The difference in means is not statistically significant.\")\n",
    "        print(\"This suggests that the two groups are not different on this variable.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae3725da6f2bfac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:34:05.551218Z",
     "start_time": "2025-09-01T17:34:05.548827Z"
    }
   },
   "outputs": [],
   "source": [
    "def chi_squared_test(df, var1, var2):\n",
    "    \"\"\"\n",
    "    Performs a chi-squared test of independence for two categorical variables.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        var1 (str): The name of the first categorical variable.\n",
    "        var2 (str): The name of the second categorical variable.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Chi-Squared Test: {var1} vs. {var2} ---\")\n",
    "\n",
    "    # Create a contingency table\n",
    "    contingency_table = pd.crosstab(df[var1], df[var2])\n",
    "    print(\"Contingency Table:\")\n",
    "    print(contingency_table)\n",
    "\n",
    "    # Perform the chi-squared test\n",
    "    chi2_stat, p_value, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "\n",
    "    # Print results and interpretation\n",
    "    print(f\"\\nChi-Squared Statistic: {chi2_stat:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "    print(f\"Degrees of Freedom: {dof}\")\n",
    "\n",
    "    alpha = 0.05\n",
    "    if p_value < alpha:\n",
    "        print(\"\\nResult: The variables are statistically associated.\")\n",
    "        print(\"This suggests there is a relationship between the two variables.\")\n",
    "    else:\n",
    "        print(\"\\nResult: The variables are not statistically associated.\")\n",
    "        print(\"This suggests there is no relationship between the two variables.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aa9578c4640942",
   "metadata": {},
   "source": [
    "## T-Test Example\n",
    "\n",
    "T-tests compare the means of groups. Our data is mostly categorical, so means aren't that interesting. Let's make a variable that counts the number of stressors someone said were somewhat or very significant so we can do some comparisons. We'll also make a binary age variable so we have just two groups to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9feead978d0fb64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:34:05.570749Z",
     "start_time": "2025-09-01T17:34:05.559063Z"
    }
   },
   "outputs": [],
   "source": [
    "variables_to_count = [col for col in df.columns if col.startswith('Q625')]\n",
    "print(f\"Found {len(variables_to_count)} variables to count: {variables_to_count}\")\n",
    "\n",
    "# Define the values to include in the count\n",
    "values_to_count = [3, 4]\n",
    "\n",
    "# Convert variables to numeric and then count how many times each value appears\n",
    "# This method handles missing values and non-numeric data gracefully\n",
    "df['stressors_count'] = df[variables_to_count].apply(pd.to_numeric, errors='coerce').isin(values_to_count).sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d50c5fc65378f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:34:05.581931Z",
     "start_time": "2025-09-01T17:34:05.579480Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a binary age variable\n",
    "df['NETAGE_binary'] = df['NETAGE'].replace({'18-24': 'Young', '25-34': 'Young',\n",
    "                                            '35-44': 'Old', '45-54': 'Old',\n",
    "                                            '55-64': 'Old', '65+': 'Old'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3b997db003382b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fdd22a2157d1579",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:34:05.594227Z",
     "start_time": "2025-09-01T17:34:05.590486Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's compare the mean of Q810R1 between the 'Young' and 'Old' age groups\n",
    "independent_t_test(df, 'stressors_count', 'NETAGE_binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3aea9a049ced55",
   "metadata": {},
   "source": [
    "# Chi-Squared Test Example\n",
    "\n",
    "We'll see if there's a relationship between the answers to these two questions:\n",
    "- Q625AR4: Please indicate how significant a source of stress it is in your life: violence toward police\n",
    "- Q3051_1: How would you rate your neighborhood on the following?: Lack of respect for rules and laws\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f437b76685ceaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:34:05.609902Z",
     "start_time": "2025-09-01T17:34:05.603269Z"
    }
   },
   "outputs": [],
   "source": [
    "# First, we need to clean and ensure they are categorical\n",
    "# Please indicate how significant a source of stress it is in your life: violence toward police\n",
    "df['Q625AR4'] = pd.to_numeric(df['Q625AR4'], errors='coerce').astype('category')  #\n",
    "# How would you rate your neighborhood on the following?: Lack of respect for rules and laws\n",
    "df['Q3051_1'] = pd.to_numeric(df['Q3051_1'], errors='coerce').astype('category')\n",
    "\n",
    "chi_squared_test(df.dropna(subset=['Q625AR4', 'Q3051_1']), 'Q625AR4', 'Q3051_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffc3a1deb830e21",
   "metadata": {},
   "source": [
    "# Linear Models\n",
    "\n",
    "We need continuous variables again for interesting analyses here. We have one for the number of stressors someone indicated. We can create another one for the big problems in their neighborhood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5659543857164",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:34:05.624977Z",
     "start_time": "2025-09-01T17:34:05.618586Z"
    }
   },
   "outputs": [],
   "source": [
    "variables_to_count = [col for col in df.columns if col.startswith('Q3051')]\n",
    "print(f\"Found {len(variables_to_count)} variables to count: {variables_to_count}\")\n",
    "\n",
    "# Define the values to include in the count\n",
    "values_to_count = [4] # only big problems\n",
    "\n",
    "# Convert variables to numeric and then count how many times each value appears\n",
    "# This method handles missing values and non-numeric data gracefully\n",
    "df['problems_count'] = df[variables_to_count].apply(pd.to_numeric, errors='coerce').isin(values_to_count).sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f8a11a94b17f2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:34:05.636988Z",
     "start_time": "2025-09-01T17:34:05.633526Z"
    }
   },
   "outputs": [],
   "source": [
    "def perform_linear_regression(df, predictor, outcome):\n",
    "    \"\"\"\n",
    "    Performs a simple linear regression and prints the results table.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        predictor (str): The name of the independent variable.\n",
    "        outcome (str): The name of the dependent (outcome) variable.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Simple Linear Regression: {outcome} vs. {predictor} ---\")\n",
    "\n",
    "    # Drop rows with missing data for the two variables\n",
    "    temp_df = df[[predictor, outcome]].dropna()\n",
    "\n",
    "    # Define the independent (X) and dependent (y) variables\n",
    "    X = temp_df[predictor]\n",
    "    y = temp_df[outcome]\n",
    "\n",
    "    # Add a constant (intercept) to the predictor variable\n",
    "    X = sm.add_constant(X)\n",
    "\n",
    "    # Create and fit the OLS model\n",
    "    model = sm.OLS(y, X)\n",
    "    results = model.fit()\n",
    "\n",
    "    # Print the regression table\n",
    "    print(results.summary())\n",
    "\n",
    "    # Extract p-value and coefficient for interpretation\n",
    "    p_value = results.pvalues[predictor]\n",
    "    coefficient = results.params[predictor]\n",
    "\n",
    "    alpha = 0.05\n",
    "    if p_value < alpha:\n",
    "        print(f\"\\nResult: There is a statistically significant relationship between {predictor} and {outcome}.\")\n",
    "        print(f\"Interpretation: For every one-point increase in {predictor}, the model predicts a {coefficient:.4f}-point change in {outcome}.\")\n",
    "    else:\n",
    "        print(f\"\\nResult: There is no statistically significant relationship between {predictor} and {outcome}.\")\n",
    "\n",
    "\n",
    "    return results\n",
    "\n",
    "def plot_scatterplot(df, x_var, y_var):\n",
    "    \"\"\"\n",
    "    Plots a scatterplot of two variables with a regression line.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the data.\n",
    "        x_var (str): The name of the variable for the x-axis.\n",
    "        y_var (str): The name of the variable for the y-axis.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Scatterplot: {y_var} vs. {x_var} ---\")\n",
    "\n",
    "    # Drop rows with missing data for the two variables\n",
    "    temp_df = df[[x_var, y_var]].dropna()\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot the scatter points\n",
    "    plt.scatter(temp_df[x_var], temp_df[y_var], alpha=0.5, label='Data Points')\n",
    "\n",
    "    # Perform linear regression to get the line\n",
    "    slope, intercept, _, _, _ = stats.linregress(temp_df[x_var], temp_df[y_var])\n",
    "\n",
    "    # Create the regression line's data\n",
    "    x_line = np.array([temp_df[x_var].min(), temp_df[x_var].max()])\n",
    "    y_line = slope * x_line + intercept\n",
    "\n",
    "    # Plot the regression line\n",
    "    plt.plot(x_line, y_line, color='red', label=f'Regression Line\\n(y = {slope:.2f}x + {intercept:.2f})')\n",
    "\n",
    "    plt.title(f'Scatterplot of {y_var} vs. {x_var}')\n",
    "    plt.xlabel(x_var)\n",
    "    plt.ylabel(y_var)\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35af42a3ad1dbd13",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:34:05.712789Z",
     "start_time": "2025-09-01T17:34:05.653057Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot first\n",
    "plot_scatterplot(df, 'problems_count', 'stressors_count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea8a3b2033cea70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-01T17:34:05.725283Z",
     "start_time": "2025-09-01T17:34:05.718253Z"
    }
   },
   "outputs": [],
   "source": [
    "# then run regression\n",
    "model = perform_linear_regression(df, 'problems_count', 'stressors_count')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
